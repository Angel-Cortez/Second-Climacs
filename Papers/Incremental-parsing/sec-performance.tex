\section{Performance of our technique}

The performance of our technique can not be stated as a single figure,
nor even as a function of the size of the buffer, simply because it
depends on several factors such as the exact structure of the buffer
contents and the way the user interacts with that contents.

Despite these difficulties, we can give some indications for certain
important special cases.  We ran these tests on a $4$-core Intel Core
processor clocked at $3.3$GHz, running SBCL version 1.3.11.

\subsection{Parsing with an empty cache}

When a buffer is first created, the cache is empty.  The buffer
contents must then be read, character by character, and the cache must
be created from the contents.

We timed this situation with a buffer containing $10000$ lines of
\commonlisp{} code.  The total time to parse was around $2$ seconds.
This result deserves some clarifications:

\begin{itemize}
\item It is very unusual to have a file of \commonlisp{} code with
  this many lines.  Most files contain less than $2000$ lines, which
  is only $1/5$ of the one in our test case.
\item This result was obtained from a very preliminary version of our
  parser.  In particular, to read a character, several generic
  functions where called, including the \texttt{stream-read-char}
  function of the Gray streams library, and then several others in
  order to access the character in the buffer.  Further optimizations
  are likely to decrease the time spent to read a single character.
\item This situation will happen only when a buffer is initially read
  into the editor.  Even very significant subsequent changes to the
  contents will still preserve large portions of the cache, so that
  the number of characters actually read will only be a tiny fraction
  of the total number of characters in the buffer.
\item All the code was compiled with a value $3$ of the \texttt{debug}
  optimize quality, and with a value of $0$ of the \texttt{speed}
  optimize quality.  We plan to investigate what parts of the code are
  critical for good performance and selectively alter the default
  optimize settings for these parts.
\item This particular case can be handled by having the parser process
  the original stream from which the buffer contents was created,
  rather than giving it the buffer protocol wrapped in a stream
  protocol after the buffer has been filled.  That way, the entire
  overhead of the Gray-stream protocol is avoided altogether.
\end{itemize}

\subsection{Parsing after small modifications}

To get an idea of performance during normal editing, we measured the
time to update the cache after a single character was added or deleted
to an existing symbol nested inside a top-level function definition.

For a small function ($10$ lines) with a nesting depth of $4$, we
measured a total time for update to around $5ms$ when the inserted or
deleted character was at the deepest nesting level, which is the least
favorable situation.

For a much larger function ($136$ lines) with a nesting depth of $12$,
the time for update varied according to the nesting level affected by
the inserted or deleted character.  For a small nesting depth, we
obtained an update time of $12ms$, and for a large nesting depth, we
obtained a time of $30ms$.

For inserting and deleting a left parenthesis, we obtained the
result shown in the table below.  For this benchmark, the performance
is independent of the size of the sub-forms of the top-level forms.
For that reason, the form size is given only in number of lines.
These are the results:

\vskip 0.3cm
\begin{tabular}{|r|r|r|}
\hline
forms & form size & time\\
\hline
120 & 10 & 1.3ms\\
80 & 15  & 1.0ms\\
60 & 20  & 0.5ms\\
40 & 30  & 0.7ms\\
30 & 40  & 0.6ms\\
24 & 50  & 0.5ms\\
12 & 100 & 0.5ms\\
\hline
\end{tabular}
\vskip 0.3cm

As the table shows, the performance is worse for many small top-level
forms, and then the execution time is roughly proportional to the
number of forms.  When the number of top-level forms is small, the
execution time decreases asymptotically to around 0.5ms.
